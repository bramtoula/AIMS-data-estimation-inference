{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.9 64-bit ('aims_data_est_inf': conda)",
   "display_name": "Python 3.6.9 64-bit ('aims_data_est_inf': conda)",
   "metadata": {
    "interpreter": {
     "hash": "fcac6859070688d07a7302584c24a0dcd69fdf30541764d7ee67340e32c9bb13"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# AIMS CDT Data, estimation and inference lab - Benjamin Ramtoula"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Setting up and preparing data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Setting up notebook"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% cd /content/\n",
    "% rm -rf AIMS-data-estimation-inference\n",
    "! git clone https://github.com/bramtoula/AIMS-data-estimation-inference.git\n",
    "% cd AIMS-data-estimation-inference\n",
    "% mkdir images\n",
    "! wget http://www.robots.ox.ac.uk/~mosb/teaching/AIMS_CDT/sotonmet.txt\n",
    "\n",
    "! apt install texlive-fonts-recommended texlive-fonts-extra cm-super dvipng\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "! pip install numpy matplotlib pandas scipy\n",
    "save_images_dir = 'images/'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [7.2,4.45]\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['xtick.labelsize'] = 16\n",
    "plt.rcParams['ytick.labelsize'] = 16\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['axes.labelsize'] = 17\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.sans-serif\": [\"Helvetica\"],\n",
    "    \"font.size\": 15})\n",
    "\n",
    "import numpy as np\n",
    "np.seterr(divide = 'ignore') "
   ]
  },
  {
   "source": [
    "## Loading and checking data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_utils import load_data_from_csv, datetime_to_hours_passed, center_readings\n",
    "data = load_data_from_csv('sotonmet.txt')"
   ]
  },
  {
   "source": [
    "Lets quickly check what we have here"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(len(data.columns)) + \" columns read:\\n\"+ str(data.columns.values)+'\\n')\n",
    "nb_readings = data.shape[0]\n",
    "print(\"Number of readings: \" + str(nb_readings) + '\\n')\n",
    "print('First reading:')\n",
    "print(data.iloc[0])\n",
    "\n",
    "print('\\nLast reading:')\n",
    "print(data.iloc[-1])"
   ]
  },
  {
   "source": [
    "Let's visualize the tide height readings, in which we are intereted, and see what they look like!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data['Reading Date and Time (ISO)'],data['Tide height (m)'],5)\n",
    "plt.title('Tide height readings')\n",
    "plt.xlabel('Reading date')\n",
    "plt.ylabel('Tide height (m)')\n",
    "plt.xticks(fontsize=12)"
   ]
  },
  {
   "source": [
    "## Preparing the data to work with"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Time to extract the data. It seems to cover about 5 days, so hours might be a good unit to rely on (time values would range from 0 to about 124 here). It might also make our life simpler to center our tide heights around 0."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "t = datetime_to_hours_passed(data['Reading Date and Time (ISO)'])\n",
    "y, removed_mean = center_readings(data['Tide height (m)']) \n",
    "y_gt = data['True tide height (m)'] - removed_mean\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "Let's just check the data is as expected: about 124 hours in the x axis and readings centered around 0m."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(t,y,5)\n",
    "plt.title('Pre-processed data to work with')\n",
    "plt.xlabel('Hours since first reading')\n",
    "plt.ylabel('Tide height difference to mean (m)')"
   ]
  },
  {
   "source": [
    "# Performing Gaussian process regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let's begin by splitting the data we have."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split valid and invalid readings from nan values\n",
    "invalid_reading_bool = y.apply(np.isnan).tolist()\n",
    "invalid_reading_indexes = np.argwhere(invalid_reading_bool)\n",
    "valid_reading_indexes = np.argwhere(np.invert(invalid_reading_bool))\n",
    "\n",
    "# Flatten indexes\n",
    "invalid_reading_indexes = [val for sublist in invalid_reading_indexes for val in sublist]\n",
    "valid_reading_indexes = [val for sublist in valid_reading_indexes for val in sublist]\n",
    "\n",
    "# Use valid readings for training and invalid for testing. Also extract groundtruth everywhere\n",
    "t_train = t[valid_reading_indexes].to_numpy().reshape(-1,1)\n",
    "y_train = y[valid_reading_indexes].to_numpy().reshape(-1,1)\n",
    "y_gt_at_train = y_gt[valid_reading_indexes].to_numpy().reshape(-1,1)\n",
    "\n",
    "t_test = t[invalid_reading_indexes].to_numpy().reshape(-1,1)\n",
    "t_all_readings = t.to_numpy().reshape(-1,1)\n",
    "y_gt_at_test = y_gt[invalid_reading_indexes].to_numpy().reshape(-1,1)\n",
    "\n",
    "# Points to evaluate to visualize overall behavior\n",
    "t_viz = np.linspace(-20,145,400).reshape(-1,1)"
   ]
  },
  {
   "source": [
    "What would be a good measurement noise?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Variance of measurement error using groundtruth: '+ str(np.var(y_train-y_gt_at_train)) + ' m^2')"
   ]
  },
  {
   "source": [
    "This is actually very small. Let's use something slightly higher just in case.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_meas_noise = 0.001"
   ]
  },
  {
   "source": [
    "## Exploring kernels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### RBF Kernel\n",
    "Let's play around with an RBF kernel and get an understanding how its results."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We first hand tune the parameters until we get something reasonable."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.gaussian_process import GaussianProcess\n",
    "\n",
    "# First parameters are sigma and lengthscale of the RBF kernel.\n",
    "gp_rbf = GaussianProcess('rbf', var_meas_noise, [(1,1.2)])\n",
    "pred_mean, pred_var, pred_sigma, pred_cov, log_marg_likelihood = gp_rbf.fit_to_data(t_train, y_train, t_viz)"
   ]
  },
  {
   "source": [
    "Let's see how that went with some visualization!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_utils import Visualizer\n",
    "visualizer = Visualizer(t_train,y_train,t_test,y_gt_at_test)\n",
    "visualizer.plot(t_viz, pred_mean, pred_sigma, pred_cov, log_marg_likelihood, nb_draws=5, title = 'RBF - Manual tuning', save_path=save_images_dir + 'rbf_manual_tune.png')"
   ]
  },
  {
   "source": [
    "The training data is fit pretty well. Without any data, we quickly jump back to a prior with high uncertainty and a distribution centered around 0.\n",
    "What if we play with the parameters?\n",
    "Let's first reduce the standard deviation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_rbf.set_covariance_params_manually([(0.1,1.2)])\n",
    "pred_mean, pred_var,pred_sigma, pred_cov, log_marg_likelihood = gp_rbf.fit_to_data(t_train, y_train, t_viz)\n",
    "visualizer.plot(t_viz, pred_mean, pred_sigma, pred_cov, log_marg_likelihood, nb_draws=5, title = 'RBF - Low $\\sigma$', save_path=save_images_dir + 'rbf_low_sigma.png')"
   ]
  },
  {
   "source": [
    "Reducing the standard deviation parameter of the RBF kernel makes the posterior predictive distribution much less uncertain when no data is present. This makes sense because the variance at testing points is directly affected by the values in the covariance function, which are now scaled to a lower value when reducing the sigma parameter of the RBF kernel.\n",
    "\n",
    "What if we decrease or increase the lengthscale?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_params = [(1,0.8)]\n",
    "gp_rbf.set_covariance_params_manually(rbf_params)\n",
    "pred_mean, pred_var, pred_sigma, pred_cov, log_marg_likelihood = gp_rbf.fit_to_data(t_train, y_train, t_viz)\n",
    "visualizer.plot(t_viz, pred_mean, pred_sigma, pred_cov, log_marg_likelihood, nb_draws=5, title = 'RBF - Lower $l$', save_path=save_images_dir + 'rbf_low_l.png')\n",
    "\n",
    "rbf_params = [(1,2)]\n",
    "gp_rbf.set_covariance_params_manually(rbf_params)\n",
    "pred_mean, pred_var, pred_sigma, pred_cov, log_marg_likelihood = gp_rbf.fit_to_data(t_train, y_train, t_viz)\n",
    "visualizer.plot(t_viz, pred_mean, pred_sigma, pred_cov, log_marg_likelihood, nb_draws=5, title = 'RBF - Higher $l$', save_path=save_images_dir + 'rbf_high_l.png')"
   ]
  },
  {
   "source": [
    "If the lengthscale is too small, we observe that for points points far from training points, the prediction can easily lead back to the prior with a mean of zero. This happens even when training data is available before and after the time of prediction. This is because having a smaller lengthscale parameter leads to smaller covariance values for training points at a given distance of a testing point. As such, Too small of a lengthscale value limits how far away we can extrapolate from training data.\n",
    "\n",
    "On the other hand, if it is too large, then data even far from testing points can affect its predictive posterior distribution. We see this in the second plot where the extrapolation does not fit well ground truth points even in close proximity from training data, because the lengthscale being so large prevents the result from favorising the effect at the local level. The variations in predictive posterior distribution are much larger and smoother, because of the larger number of points having a significant contribution to each estimate."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Overall, with the appropriate parameters, it seems like an RBF kernel can perform well close to training data. However, far from the data the prior does not fit well the tide height pattern seen here.\n",
    "\n",
    "This hints that a periodic kernel might help take advantage of the periodicity of tides that we can observe here."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Periodic kernel"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let's see how a periodic function would fit our data.\n",
    "We can take advantage of knowledge that tides happen every 12 hours and 25 minutes to set the period of our kernel. We can tune by hand other parameters until we get satisfying results."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_periodic = GaussianProcess('periodic', var_meas_noise, [(0.5,0.1,12.42)])\n",
    "\n",
    "pred_mean, pred_var, pred_sigma, pred_cov, log_marg_likelihood = gp_periodic.fit_to_data(t_train, y_train, t_viz)\n",
    "visualizer.plot(t_viz, pred_mean, pred_sigma, pred_cov, log_marg_likelihood, nb_draws=5, title = 'Periodic - Manual tuning', save_path=save_images_dir + 'periodic_manual_tune.png')"
   ]
  },
  {
   "source": [
    "What happens if we play with the other parameters?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "periodic_params = [(0.5, 0.02, 12.42)]\n",
    "gp_periodic.set_covariance_params_manually(periodic_params)\n",
    "pred_mean, pred_var, pred_sigma, pred_cov, log_marg_likelihood = gp_periodic.fit_to_data(t_train, y_train, t_viz)\n",
    "visualizer.plot(t_viz, pred_mean, pred_sigma, pred_cov, log_marg_likelihood, nb_draws=5, title = 'Periodic - Lower $l$', save_path=save_images_dir + 'periodic_low_l.png')"
   ]
  },
  {
   "source": [
    "The lengthscale being too small leads to relying a lot more on the closest measurements, which result in noisier estimates."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "With a periodic kernel, we can do a good job of following the general pattern of the tide heights, even a times far from measurements. However, the posterior predictive distribution does not fit the training data as well as the RBF kernel, and has too low of an uncertainty."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Rational quadratic kernel"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Other kernels to consider include the Matérn class of covariances, the affine covariance, or the rational quadratic covariance for example.\n",
    "The Matérn class might not be required here because the data is quite smooth. The affine covariance would not be helpful because the data would not be suited well to an affine model. We expect the tide heights to remain similar over longs periods of times, so a non-stationary covariance does not seem required.\n",
    "\n",
    "However, the rational quadratic kernel might allow a smooth fit, so it's worth exploring it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat_quad_params = [(1, 1, 1.5)]\n",
    "gp_rat_quad = GaussianProcess('rat_quad', var_meas_noise, rat_quad_params)\n",
    "\n",
    "pred_mean, pred_var, pred_sigma, pred_cov, log_marg_likelihood = gp_rat_quad.fit_to_data(t_train, y_train, t_viz)\n",
    "visualizer.plot(t_viz, pred_mean, pred_sigma, pred_cov, log_marg_likelihood, nb_draws=5, title = 'Rational Quadratic - Manual tuning', save_path=save_images_dir + 'rat_quad_manual_tune.png')"
   ]
  },
  {
   "source": [
    "This looks actually fairly similar to results obtained with an RBF kernel. It fits well the training data but does not perform well far from it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Optimising hyperparameters\n",
    "\n",
    "Now let's try to find the best parameters for each kernel choice. We do this by optimising for the minimum negative log marginal likelihood."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### RBF kernel"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rbf_params_bounds = [(0,None),(1e-10,None)]\n",
    "gp_rbf.set_covariance_params_optimize(t_train, y_train,rbf_params,rbf_params_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mean, pred_var, pred_sigma, pred_cov, log_marg_likelihood = gp_rbf.fit_to_data(t_train, y_train, t_viz)\n",
    "visualizer.plot(t_viz, pred_mean, pred_sigma, pred_cov, log_marg_likelihood, nb_draws=5, title = 'RBF - Optimized parameters', save_path = save_images_dir + 'rbf_optim.png')"
   ]
  },
  {
   "source": [
    "### Periodic kernel"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We are keeping the period fixed and only optimizing on the variance and the lengthscale\n",
    "periodic_params_bounds = [(0,None),(1e-8,None)]\n",
    "gp_periodic.set_covariance_params_optimize(t_train, y_train,[periodic_params[0][:2]],periodic_params_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mean, pred_var, pred_sigma, pred_cov, log_marg_likelihood = gp_periodic.fit_to_data(t_train, y_train, t_viz)\n",
    "visualizer.plot(t_viz, pred_mean, pred_sigma, pred_cov, log_marg_likelihood, nb_draws=5, title = 'Periodic - Optimized parameters', save_path = save_images_dir + 'periodic_optim.png')"
   ]
  },
  {
   "source": [
    "The minimization of the negative log marginal likelihood does not seem to be a good choice for the periodic kernel."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Rational quadratic kernel"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rat_quad_params_bounds = [(0,None),(1e-5,None),(1e-5,None)]\n",
    "gp_rat_quad.set_covariance_params_optimize(t_train, y_train,rat_quad_params,rat_quad_params_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mean, pred_var, pred_sigma, pred_cov, log_marg_likelihood = gp_rat_quad.fit_to_data(t_train, y_train, t_viz)\n",
    "visualizer.plot(t_viz, pred_mean, pred_sigma, pred_cov, log_marg_likelihood, nb_draws=5, title = 'Rational Quadractic - Optimized parameters', save_path = save_images_dir + 'rat_quad_optim.png')"
   ]
  },
  {
   "source": [
    "The results with the rational quadratic kernel look slightly better than with an RBF kernel, but still fail far fromt training data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Finding the best covariance function\n",
    "We can try to find the best covariance function by optimising parameters of a combination of additions and multiplication of kernels with multipliers applied to each term."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build initial parameters and bounds of the covariance function to use for the optimisation. It assumes the same set of parameters for each kernel.\n",
    "def get_init_params_and_bounds_from_kernel_str(kernel_str,init_rbf_params, init_periodic_params, init_rat_quad_params, bounds_rbf = [(0,None),(1e-8,None)], bounds_periodic = [(0,None),(1e-8,None)], bounds_rat_quad = [(0,None),(1e-3,None),(1e-3,None)], bounds_term_coeffs = [(0,None)]):\n",
    "    init_kernel_params = []\n",
    "    bounds_kernel_params = []\n",
    "    init_multipliers = []\n",
    "    bounds_multipliers = []\n",
    "    nb_terms = 0\n",
    "    for k in kernel_str.split(' '):\n",
    "        if k == 'rbf':\n",
    "            init_kernel_params+=init_rbf_params\n",
    "            bounds_kernel_params+=bounds_rbf\n",
    "            nb_terms+=1\n",
    "        elif k == 'periodic':\n",
    "            init_kernel_params+=init_periodic_params\n",
    "            bounds_kernel_params+=bounds_periodic\n",
    "            nb_terms+=1\n",
    "        if k == 'rat_quad':\n",
    "            init_kernel_params+=init_rat_quad_params\n",
    "            bounds_kernel_params+=bounds_rat_quad\n",
    "            nb_terms+=1\n",
    "    init_multipliers = [1/nb_terms]*nb_terms\n",
    "    bounds_multipliers = bounds_term_coeffs*nb_terms\n",
    "    return init_kernel_params, bounds_kernel_params, init_multipliers, bounds_multipliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a combination of every possible term composed of the kernels considered.\n",
    "kernel_combination = 'rbf + periodic + rat_quad + rbf * periodic * rat_quad + rbf * rat_quad + rbf * periodic + periodic * rat_quad'\n",
    "\n",
    "gp_combine_kernels = GaussianProcess(kernel_combination, var_meas_noise)\n",
    "\n",
    "init_kernel_params, bounds_kernel_params, init_multipliers, bounds_multipliers = get_init_params_and_bounds_from_kernel_str(kernel_combination,rbf_params, [periodic_params[0][:2]], rat_quad_params)\n",
    "gp_combine_kernels.set_covariance_params_optimize(t_train, y_train,init_kernel_params,bounds_kernel_params,init_multipliers,bounds_multipliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mean, pred_var, pred_sigma, pred_cov, log_marg_likelihood = gp_combine_kernels.fit_to_data(t_train, y_train, t_viz)\n",
    "visualizer.plot(t_viz, pred_mean, pred_sigma, pred_cov, log_marg_likelihood, nb_draws=5, title = 'Combined kernels - Optimized parameters', save_path = save_images_dir + 'combined_optim.png')"
   ]
  },
  {
   "source": [
    "The posterior predictive distribution fits well the training data but the ground truth data isn't always likely to be sampled. Let's try adjusting the parameters by hand to fix that issue."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in gp_combine_kernels.covariance_computer.kernels:\n",
    "    print(term.type)\n",
    "    print(term.params)\n",
    "    print('\\n')\n",
    "\n",
    "print(gp_combine_kernels.covariance_computer.kernel_term_multipliers)"
   ]
  },
  {
   "source": [
    "Now we get an idea of the scales and how much to play with them, as well as which terms are important (the last list of numbers are multipliers to each kernel).\n",
    "\n",
    "The resulting covariance function only has a periodic kernel added to a rational quadratic kernel.\n",
    "\n",
    "We can try to improve our results by increasing the $\\sigma$ parameter of the rational quadratic kernel, increasing the uncertainties and making ground truth data reasonable samples of our posterior predictive distribution."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "gp_combine_kernels_more_uncertain = copy.deepcopy(gp_combine_kernels)\n",
    "\n",
    "for i,term in enumerate(gp_combine_kernels_more_uncertain.covariance_computer.kernels):\n",
    "    if term.type == 'rat_quad':\n",
    "        # add 2 to the variance of the rational quadractic terms\n",
    "        gp_combine_kernels_more_uncertain.covariance_computer.add_to_kernel_param(i,0,2)\n",
    "\n",
    "pred_mean, pred_var, pred_sigma, pred_cov, log_marg_likelihood = gp_combine_kernels_more_uncertain.fit_to_data(t_train, y_train, t_viz)\n",
    "visualizer.plot(t_viz, pred_mean, pred_sigma, pred_cov, log_marg_likelihood, nb_draws=5, title = 'Combined kernels - Manual adjusted parameters', save_path = save_images_dir + 'combined_adjusted.png')"
   ]
  },
  {
   "source": [
    "## Exploring the effect of having less data\n",
    "Here, we try to get an idea of how our predictions are affected by the amount of data we have available."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We first try to use our already tuned Gaussian process with a subset of the dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_num = 10\n",
    "\n",
    "t_train_split = np.array_split(t_train, split_num)\n",
    "y_train_split = np.array_split(y_train, split_num)\n",
    "y_gt_at_train_split = np.array_split(y_gt_at_train, split_num)\n",
    "\n",
    "for i in range(split_num):\n",
    "    t_test_cur = np.concatenate([t_test]+t_train_split[i+1:])\n",
    "    y_test_cur = np.concatenate([y_gt_at_test]+y_gt_at_train_split[i+1:])\n",
    "    t_train_cur = np.concatenate(t_train_split[:i+1])\n",
    "    y_train_cur = np.concatenate(y_train_split[:i+1])\n",
    "\n",
    "    visualizer = Visualizer(t_train_cur,y_train_cur,t_test_cur,y_test_cur)\n",
    "\n",
    "    pred_mean, pred_var, pred_sigma, pred_cov, log_marg_likelihood = gp_combine_kernels_more_uncertain.fit_to_data(t_train_cur, y_train_cur, t_viz)\n",
    "\n",
    "    visualizer.plot(t_viz, pred_mean, pred_sigma, pred_cov, log_marg_likelihood, nb_draws=5, title = 'Pre-tuned GP with '+str((i+1)*100/split_num)+'\\% of data', save_path=save_images_dir + 'split_pretuned_'+str(i)+'.png')\n",
    "\n",
    "# Save gif \n",
    "import imageio\n",
    "images = []\n",
    "for i in range(split_num):\n",
    "    images.append(imageio.imread(save_images_dir + 'split_pretuned_'+str(i)+'.png'))\n",
    "imageio.mimsave(save_images_dir + 'split_pretuned_all.gif', images,duration=0.5)"
   ]
  },
  {
   "source": [
    "These are usually pretty good, but it is expected with a properly tuned model.\n",
    "\n",
    "How about if we try tuning our model only with the subset of data?\n",
    "We try doing so by keeping the same covariance function obtained as a result of the previous optimisation, one composed of the sum of a periodic and a rational quadratic kernel.\n",
    "It would be interesting to optimise for the complete covariance function again, with multipliers in front of each possible term, but this is computationally expensive.\n",
    "\n",
    "This time, we initialize our parameters with random values to limit the bias obtained by working with the full dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_random_init_params_and_bounds_from_kernel_str(kernel_str,random_range = [1e-5,5], bounds_rbf = [(0,None),(1e-8,None)], bounds_periodic = [(0,None),(1e-8,None)], bounds_rat_quad = [(0,None),(1e-3,None),(1e-3,None)], bounds_term_coeffs = [(0,None)]):\n",
    "    init_kernel_params = []\n",
    "    bounds_kernel_params = []\n",
    "    init_multipliers = []\n",
    "    bounds_multipliers = []\n",
    "    nb_terms = 0\n",
    "    for k in kernel_str.split(' '):\n",
    "        if k == 'rbf':\n",
    "            init_kernel_params+= [(random.uniform(random_range[0],random_range[1]), random.uniform(random_range[0],random_range[1]))]\n",
    "            bounds_kernel_params+=bounds_rbf\n",
    "            nb_terms+=1\n",
    "        elif k == 'periodic':\n",
    "            init_kernel_params+=[(random.uniform(random_range[0],random_range[1]), random.uniform(random_range[0],random_range[1]))]\n",
    "            bounds_kernel_params+=bounds_periodic\n",
    "            nb_terms+=1\n",
    "        if k == 'rat_quad':\n",
    "            init_kernel_params+=[(random.uniform(random_range[0],random_range[1]), random.uniform(random_range[0],random_range[1]), random.uniform(random_range[0],random_range[1]))]\n",
    "            bounds_kernel_params+=bounds_rat_quad\n",
    "            nb_terms+=1\n",
    "    init_multipliers = [1/nb_terms]*nb_terms\n",
    "    bounds_multipliers = bounds_term_coeffs*nb_terms\n",
    "    return init_kernel_params, bounds_kernel_params, init_multipliers, bounds_multipliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_num = 20\n",
    "\n",
    "t_train_split = np.array_split(t_train, split_num)\n",
    "y_train_split = np.array_split(y_train, split_num)\n",
    "y_gt_at_train_split = np.array_split(y_gt_at_train, split_num)\n",
    "\n",
    "kernels_split = 'periodic + rat_quad'\n",
    "gp_retrain_split = GaussianProcess(kernels_split, var_meas_noise)\n",
    "\n",
    "# Start with random parameters\n",
    "init_kernel_params, bounds_kernel_params, init_multipliers, bounds_multipliers = get_random_init_params_and_bounds_from_kernel_str(kernels_split)\n",
    "\n",
    "\n",
    "for i in range(split_num):\n",
    "    t_test_cur = np.concatenate([t_test]+t_train_split[i+1:])\n",
    "    y_test_cur = np.concatenate([y_gt_at_test]+y_gt_at_train_split[i+1:])\n",
    "    t_train_cur = np.concatenate(t_train_split[:i+1])\n",
    "    y_train_cur = np.concatenate(y_train_split[:i+1])\n",
    "\n",
    "    visualizer = Visualizer(t_train_cur,y_train_cur,t_test_cur,y_test_cur)\n",
    "\n",
    "    # Optimize parameters and store results to initialize next o\n",
    "    init_kernel_params, init_multipliers = gp_retrain_split.set_covariance_params_optimize(t_train, y_train,init_kernel_params,bounds_kernel_params,init_multipliers,bounds_multipliers)\n",
    "\n",
    "    pred_mean, pred_var, pred_sigma, pred_cov, log_marg_likelihood = gp_retrain_split.fit_to_data(t_train_cur, y_train_cur, t_viz)\n",
    "\n",
    "    visualizer.plot(t_viz, pred_mean, pred_sigma, pred_cov, log_marg_likelihood, nb_draws=5, title = 'Tuned GP with '+str((i+1)*100/split_num)+'\\% of data', save_path=save_images_dir + 'split_tune_'+str(i)+'.png')\n",
    "# Save gif \n",
    "import imageio\n",
    "images = []\n",
    "for i in range(split_num):\n",
    "    images.append(imageio.imread(save_images_dir + 'split_tune_'+str(i)+'.png'))\n",
    "imageio.mimsave(save_images_dir + 'split_tune_all.gif', images,duration=0.5)"
   ]
  },
  {
   "source": [
    "With little data, it is harder to get a good set of parameters to fit the whole dataset. This is especially the case when a full period is not observed.\n",
    "\n",
    "The inclusion of prior knowledge through the choice of the covariance function and the setting of the period parameter of the periodic kernel still help to have reasonable results."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
